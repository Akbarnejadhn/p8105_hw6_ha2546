---
title: "p8105_hw6_ha2546"
author: "Hana Akbarnejad"
date: "11/27/2019"
output:
  github_document:
   pandoc_args: --webtex
---

```{r setup, include=FALSE}

library(tidyverse)
library(rvest)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(1)
```

# Problem 1

## Load and clean data

```{r}

birthweight_data = read_csv("data/birthweight.csv") %>% 
  rename_at(vars(starts_with ("b")), 
  funs(str_replace(., "b", "baby_"))
  ) %>% 
  rename_at(vars(starts_with ("m")), 
  funs(str_replace(., "m", "mother_"))
  ) %>%
  rename_all(
  funs(str_replace(., "wt", "weight"))    
  ) %>% 
  rename(
    baby_sex = baby_abysex,
    father_race = frace,
    family_income = fincome,
    mother_delivery_weight = delweight,
    gestational_age = gaweeks,
    malformation = mother_alform,
    mother_menarche = mother_enarche,
    mother_delivery_age= mother_omage,
    past_live_birth = parity,
    past_low_birth = pnumlbw,
    past_small_birth = pnumsga,
    pre_preg_bmi = ppbmi,
    pre_preg_weight = ppweight,
    weight_gain = weightgain
  ) %>% 
  mutate(
    baby_sex = factor(baby_sex),
    father_race = factor(father_race),
    malformation = factor(malformation),
    mother_race = factor(mother_race)
  )

sum(is.na(birthweight_data))
```

In this part, I loaded and cleaned the data. I renamed variables in a way that are more comprehendible, and factored the numeric variables. Using _is.na_ function, and taking the sum, no missing values found in the datset. The _birthweight_data_ is a dataset of `r nrow(birthweight_data)` observations of `r ncol(birthweight_data)` variables that might be associated with babyweight, such as babies sex and height, race/ethnicity of parents, family income, and informations about mother's age, weight, bmi, etc, both pre pregnancy and at the time of delivery. The birthweight of babies that is our dependent variable of interest in these problems (response), has the mean of `r pull(birthweight_data, baby_weight) %>%  mean()` $\pm$ `r pull(birthweight_data, baby_weight) %>%  sd()` grams or `r pull(birthweight_data, baby_weight) %>%  mean() / 1000` $\pm$ `r pull(birthweight_data, baby_weight) %>%  sd() /1000` kilograms, with median of `r pull(birthweight_data, baby_weight) %>%  median()` gr (`r pull(birthweight_data, baby_weight) %>%  median()/1000` kg).

The below plots shows the distribution of birth weight:
```{r}

birthweight_data %>% 
  ggplot(aes(x = baby_weight)) + geom_density()
```
Although a little skewed to the left, the data is normally distributed.

## Propose Regression Model

The purose of this part is to propose a regression model for birthweight. Since the sample size is huge, it is going to be hard to build a model because in such big datasets, almost all variable show significance, so using stepwise approaches would not be very helpful. So, my model is mostly relied on literature review and then I check the validity of my model.

As we are looking for a predictive model, I first start with excluding factors that I don't find very relevant, and/or have not been mentioned in previous literature, or have shown to have non-signiicant effects on baby birthweight. The factors thaat are left to consider are: _baby_head_, _babylength_, _gestational_age_, _prepreg_bmi_, _malformation_, _smoken_.


Now that I reduced 19 predictors to 6, I include factors that are logically associated with birthweight of babies and are related to babies and not other factors. Babies with higher gestational age are going to be heavier and heavier babies would have bigger heads, and would be generally "bigger", so they would have higher height. Since all these factors are related to each other, I would use interaction term between thse three factors. So, my first proposed model is:
```{r}

fit1 = birthweight_data %>% 
  lm(baby_weight ~ baby_head*baby_length*gestational_age, data = .) 

fit1 %>% 
  summary()
```

Now, I add smoking status of mother to the model:

```{r}

fit2 = birthweight_data %>% 
  lm(baby_weight ~ baby_head*baby_length*gestational_age + smoken, data = .) 

fit2 %>% 
  summary()
```

since adding smoking status has a very negligible effect on $r^2$, and its p-value is the highest between all other values, I omit that from my model, and add malformation to my model:

```{r}

fit3 = birthweight_data %>% 
  lm(baby_weight ~ baby_head*baby_length*gestational_age + malformation, data = .) 

fit3 %>% 
  summary()
```

We can observe that malformation effect is not significant in our model and it has a high p-value compared to other terms. So, I remove that factor from my model and go with the last predictor that I think might be helpful, which is _prepreg_bmi_:

```{r}

fit4 = birthweight_data %>% 
  lm(baby_weight ~ baby_head*baby_length*gestational_age + pre_preg_bmi, data = .) 

fit4 %>% 
  summary()
```

This factor as well is not very significant and does not considerable improve the value of adjusted $r^2$. So my final model is the first version of my model which was named _fit1_.

## Model Diagnostics
Next, I should plot the residuals against fitted values to check the constant variance assumption and ensure validity of my model.
```{r}

birthweight_data %>% 
  add_residuals(fit1) %>% 
  add_predictions(fit1) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() + geom_line(y = 0)
```

We can observe randomness which shows homoscedasticity (constant variance) in residuals. This indicates that my model is correctly specified and there is no corrolation between residuals and fitted values. So, there is no need for transformations in my outcome or predictor, and that the linear model is working well, so, we do not need to consider higher order regression models. The only concern is the few points that look like outliers. we can look into them to see if these outliers are influential points or not, but because our sample size is very big, the presence of these points should not be problematic.

## Model Comparison, Cross Validation

first lets just see the models
```{r}

model1 = birthweight_data %>% 
  lm(baby_weight ~ baby_length + gestational_age, data = .)

model1 %>% 
  broom::tidy()

model2 = birthweight_data %>% 
  lm(baby_weight ~ baby_length*baby_head*baby_sex, data = .)

model2 %>% 
  broom::tidy()
```

I used automated cross calidation process using modelr...
```{r}

# train/test split using resampling
cv_df = 
  crossv_mc(birthweight_data, 100) 

# conversting train and test lists to tibbles (I assume we don't have to because all lm???)
cv_df = cv_df %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
    )

# obtaining RMSE of our models:
cv_df = cv_df %>% 
  mutate(
    fit1  = map(train, ~lm(baby_weight ~ baby_head*baby_length*gestational_age, data = .)),
    model1  = map(train, ~lm(baby_weight ~ baby_length + gestational_age, data = .x)),
    model2  = map(train, ~lm(baby_weight ~ baby_length * baby_head * baby_sex, data = .x))) %>% 
  mutate(
    rmse_fit1 = map2_dbl(fit1, test, ~rmse(model = .x, data = .y)),
    rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)))
```

Then, I used RMSE as a way to compare these models, and the plot below shows the distribution of RMSE values for each candidate model (fit1, model1, model2):

```{r}

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

The above violin plots show that both model2 and my proposed model are good predictive models in terms of RMSE. There is not much difference between these models and the proposed model has only slightly decreased RMSE, meaning that the presnce of _gestational age_ instead on _sex_ has been benefitial to our model.

# Problem 2

loadind weather data:

```{r}

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

fit a linear regression with tmax as response and tmin as predictor:

```{r}

weather_model = weather_df %>% 
  lm(tmax ~ tmin, data = .)

weather_model %>%
  broom::tidy() %>% 
  knitr::kable()
```

Now, we want to use bootsrap to make inference about this data!

First step is writing bootstrap function (to draw one sample with replacement), and then, I apply the function to draw 5000 samples:

```{r}

boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )
```

for each bootstrap sample, produce estimates of $\hat{r}^2$ and $log(\hat{\beta_0} * \hat{\beta_1})$ quantities.
first, we should extract values of $\hat{\beta_0}$, $\hat{\beta_1}$, and  $\hat{r}^2$ from the linear model that we have, using broom::tidy() and broom::glance().

```{r}

boot_analysis = boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),   
    results_coeff = map(models, broom::tidy), # to get coefficients                     
    results_r2 = map(models, broom::glance) # to get r^2
  ) %>%
  select(-strap_sample, -models) %>% 
  unnest() %>% 
  select(strap_number, term, estimate, r.squared) # selecting variables I'm gonna use

# to extract coefficients from bootstrap results and take the log:
boot_estimates = boot_analysis %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(
    beta0 = "(Intercept)",
    beta1 = "tmin",
    r_squared = "r.squared"
  ) %>% 
  mutate(
    log_coeff = log(beta0*beta1)
  )
```

Next step is graphing the distribution of our estimates that are obtained:

```{r}

# plotting the distribution of estimates:
boot_estimates %>% 
  ggplot(aes(x = log_coeff)) + geom_density() +
  labs(
    x = "log(beta0 * beta1)",
    y = "Density",
    title = "Distribution of log(beta0 * beta1) estimate, obtained from bootstrapping",
    caption = "Results obtained from 5000 sample bootstrap"
  )

boot_estimates %>% 
  ggplot(aes(x = r_squared)) + geom_density() +
  labs(
    x = "r-squared",
    y = "Density",
    title = "Distribution of r-squared estimate, obtained from bootstrapping",
    caption = "Results obtained from 5000 sample bootstrap"
  )
```

#################### comment on distributions!

In this part, I identified the 2.5% and 97.5% quantiles to provide a 95% confidence interval for $\hat{r}^2$ and $log(\hat{\beta_0} * \hat{\beta_1})$

```{r}

quantile(pull(boot_estimates, log_coeff), c(0.25, 0.75))

quantile(pull(boot_estimates, r_squared), c(0.25, 0.75))
```

We can see that 95% confidence interval is (1.997025, 2.029787) for $log(\hat{\beta_0} * \hat{\beta_1})$ and (0.9056913 , 0.9171143) for $\hat{r}^2$.